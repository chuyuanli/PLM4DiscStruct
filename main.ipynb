{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, BartModel, BertModel, MBartModel\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import sys\n",
    "from IPython.core.debugger import set_trace #set_trace()\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate EDU aggregated self-attention Matrixes from BART etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, model_params):\n",
    "        self.model_params = model_params\n",
    "        self.files = [os.path.join(file_path, x) for x in os.listdir(file_path) if x.endswith(\".out.edus\")]\n",
    "        if 'cli' in model_params[\"name\"]: #fine-tuned on sentence ordering, models stored in ReBART/ repo\n",
    "            model_path = os.path.join(\"ReBART/outputs/\", model_params[\"name\"].split('cli/')[-1])\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_params[\"name\"], \n",
    "                                                       cache_dir=\"./huggingface_model_files\",\n",
    "                                                       local_files_only=True)    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        edu_lengths = []\n",
    "        inputs = []\n",
    "        slice_list = []\n",
    "        with open(self.files[index], \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip().lower().replace(\"\\n\", \"\").replace(\"\\r\", \"\") + \" \"\n",
    "                tokens = self.tokenizer(line)[\"input_ids\"][self.model_params[\"prefix_len\"]:-self.model_params[\"postfix_len\"]]\n",
    "                edu_lengths.append(len(tokens))\n",
    "                inputs.extend(tokens)\n",
    "\n",
    "        if len(inputs) <= self.model_params[\"max_input_size\"]:\n",
    "            slice_list = [torch.tensor([inputs])]    \n",
    "        else:\n",
    "            # Split input into slices of the max model input size and add start+end tokens\n",
    "            for split_point in range(len(inputs)-(self.model_params[\"max_input_size\"]-2)+1):\n",
    "                curr_inputs = copy.deepcopy(inputs)[split_point:split_point+(self.model_params[\"max_input_size\"]-2)]\n",
    "                curr_inputs = [self.model_params[\"start_token\"]] + curr_inputs + [self.model_params[\"end_token\"]]\n",
    "                slice_list.append(curr_inputs)\n",
    "            if len(slice_list) <= self.model_params[\"batch_size\"]:\n",
    "                slice_list = [torch.tensor(slice_list)]\n",
    "            else:\n",
    "                slice_list = torch.tensor(slice_list)\n",
    "                slice_list = list(torch.split(slice_list, self.model_params[\"batch_size\"]))\n",
    "\n",
    "        # Generate huggingface required format for model inputs\n",
    "        data_objects = []\n",
    "        for data_slice in slice_list:\n",
    "            datapoint = {}\n",
    "            datapoint[\"input_ids\"] = data_slice\n",
    "            datapoint[\"attention_mask\"] = torch.ones(data_slice.size(), dtype=torch.long)\n",
    "            if self.model_params[\"has_token_type_ids\"]:\n",
    "                datapoint[\"token_type_ids\"] = torch.zeros(data_slice.size(), dtype=torch.long)\n",
    "            data_objects.append(datapoint)\n",
    "        file_name = os.path.basename(self.files[index]).split(\".\")[0]\n",
    "        return data_objects, len(inputs), edu_lengths, file_name, self.files[index]\n",
    "\n",
    "def squeeze_if_needed(data_slice):\n",
    "    if len(data_slice.size()) == 3:\n",
    "        data_slice = data_slice.squeeze(0)\n",
    "    return data_slice\n",
    "    \n",
    "def execution(gpu_core, model_params, file_path, base_save_path, layer, head, dataname, random_init, orig_parseval):\n",
    "    dataset = DiscDataset(file_path, model_params)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, shuffle=True)\n",
    "    model = None\n",
    "    start_time_all = time.time()\n",
    "    results = []\n",
    "    for datapoint in data_loader:\n",
    "        data, dimensions, edu_lengths, file_name, file_full_path = datapoint\n",
    "        file_name = file_name[0] # Unwrap the added tupel dimension\n",
    "        file_full_path = file_full_path[0] # Unwrap the added tupel dimension\n",
    "        curr_save_path = os.path.join(base_save_path, model_params[\"name\"], \"attention_matrices\", \n",
    "                                      str(layer)+\"_\"+str(head)+\"_\"+file_name+\".pt\")\n",
    "        if os.path.exists(curr_save_path):\n",
    "            edu_attn_matrix = torch.load(curr_save_path)\n",
    "        else:\n",
    "            if model is None:\n",
    "                if random_init:\n",
    "                    model = model_params[\"model\"](AutoConfig.from_pretrained(model_params[\"name\"], \n",
    "                                                                             cache_dir=\"./huggingface_model_files\", \n",
    "                                                                             output_attentions=True,\n",
    "                                                                             local_files_only=True))\n",
    "                else:\n",
    "                    model = AutoModel.from_pretrained(model_params[\"name\"], \n",
    "                                                      cache_dir=\"./huggingface_model_files\", \n",
    "                                                      output_attentions=True,\n",
    "                                                      local_files_only=True)\n",
    "                device = torch.device(f'cuda:{gpu_core}' if torch.cuda.is_available() else 'cpu')\n",
    "                model.to(device)\n",
    "                model.eval()\n",
    "            agg_self_attention = torch.zeros([dimensions, dimensions])\n",
    "            agg_self_attention_divisor = torch.zeros([dimensions, dimensions])\n",
    "            aggregated_outputs = []\n",
    "            for data_slice in data:   \n",
    "                # Feed data_slice into model\n",
    "                data_slice[\"input_ids\"] =  squeeze_if_needed(data_slice[\"input_ids\"].to(device))\n",
    "                data_slice[\"attention_mask\"] = squeeze_if_needed(data_slice[\"attention_mask\"].to(device))\n",
    "                if model_params[\"has_token_type_ids\"]:\n",
    "                    data_slice[\"token_type_ids\"] = squeeze_if_needed(data_slice[\"token_type_ids\"].to(device))\n",
    "                output = model(**data_slice)\n",
    "                output = output[model_params[\"attn_keyword\"]]\n",
    "                output = output[layer].detach()\n",
    "                output = output[:,head,:,:]\n",
    "                # Remove first and last elements, since they're not part of the input document for each batch element\n",
    "                output = output[:,1:-1, 1:-1]\n",
    "                # Aggregate self-attention outputs into a single list\n",
    "                aggregated_outputs.extend(list(output.cpu()))\n",
    "            # Average accross layovers\n",
    "            for idx, attention_slice in enumerate(aggregated_outputs):\n",
    "                aggregated_attention = torch.add(agg_self_attention[idx:idx+attention_slice.size(0), idx:idx+attention_slice.size(1)], attention_slice)\n",
    "                agg_self_attention[idx:idx+attention_slice.size(0), idx:idx+attention_slice.size(1)] = aggregated_attention\n",
    "                agg_self_attention_divisor[idx:idx+attention_slice.size(0), idx:idx+attention_slice.size(1)] += torch.ones(aggregated_attention.size())\n",
    "            agg_self_attention = agg_self_attention/agg_self_attention_divisor\n",
    "            # Combine EDUs \n",
    "            assert (sum(edu_lengths) == agg_self_attention.size(0)), \"EDUs don't match with tokens\"\n",
    "            agg_self_attention = torch.nan_to_num(agg_self_attention)\n",
    "            edu_attn_matrix = torch.eye(len(edu_lengths))\n",
    "            first_index_agg = 0 # offset from start\n",
    "            for first_idx, item1 in enumerate(edu_lengths):\n",
    "                second_index_agg = 0 # offset from start\n",
    "                for second_idx, item2 in enumerate(edu_lengths):\n",
    "                    if first_idx == second_idx: \n",
    "                        # Get outgoing importance\n",
    "                        edu_importance_pre = agg_self_attention[:first_index_agg, first_index_agg:first_index_agg+item1]\n",
    "                        edu_importance_post = agg_self_attention[first_index_agg+item1:, first_index_agg:first_index_agg+item1]\n",
    "                        edu_importance = torch.cat([edu_importance_pre, edu_importance_post])\n",
    "                        edu_attn_matrix[first_idx, second_idx] = torch.mean(torch.flatten(edu_importance))\n",
    "                        second_index_agg += item2\n",
    "                        continue\n",
    "                    else:\n",
    "                        # Make sure to not double count\n",
    "                        if float(edu_attn_matrix[first_idx, second_idx]) == 0. or float(edu_attn_matrix[second_idx, first_idx]) == 0.:\n",
    "                            # Incoming\n",
    "                            ingoing_subtable = agg_self_attention[first_index_agg:first_index_agg+item1, second_index_agg:second_index_agg+item2]\n",
    "                            ingoing_subtable = torch.mean(torch.flatten(ingoing_subtable))\n",
    "                            # Outgoing\n",
    "                            outgoing_subtable = agg_self_attention[second_index_agg:second_index_agg+item2, first_index_agg:first_index_agg+item1]\n",
    "                            outgoing_subtable = torch.mean(torch.flatten(outgoing_subtable))\n",
    "                            # Save\n",
    "                            edu_attn_matrix[first_idx, second_idx] = ingoing_subtable\n",
    "                            edu_attn_matrix[second_idx, first_idx] = outgoing_subtable\n",
    "                    second_index_agg += item2\n",
    "                first_index_agg += item1\n",
    "            torch.save(edu_attn_matrix, curr_save_path)\n",
    "        \n",
    "        # edu_attn_matrix loaded or generated\n",
    "        base_data = {\"layer\": layer, \n",
    "                     \"head\": head, \n",
    "                     \"file_name\": file_name}\n",
    "        \n",
    "        mat_size = edu_attn_matrix.size(0) #edu_attn matrix shape [n,n], n is nb of edu\n",
    "        # li: modify edu_attn_matrix to reduce backdward links, only take half right upper half, the other half give 0\n",
    "        half_edu_attn_matrix = torch.zeros(mat_size, mat_size)\n",
    "        for i in range(mat_size):\n",
    "            for j in range(mat_size):\n",
    "                if i <= j:\n",
    "                    half_edu_attn_matrix[i,j] = edu_attn_matrix[i,j]\n",
    "        \n",
    "        eisner_results = exec_eisner(file_full_path, half_edu_attn_matrix, base_save_path, model_params[\"name\"], layer, head, dataname)\n",
    "        results.append({**base_data, **eisner_results})\n",
    "    print(f\"--- {(time.time() - start_time_all)} seconds to complete head/layer combo ---\\n\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Dep Tree with Eisner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eisner(G):\n",
    "    num_v = G.size(0)\n",
    "    weight_matrix=torch.zeros(num_v,num_v,2,2)\n",
    "    selection_matrix = torch.zeros(num_v,num_v,2,2)\n",
    "    for m in range(1,num_v):\n",
    "        for i in range(num_v-m):\n",
    "            j=i+m\n",
    "            ##d=0, c=0\n",
    "            max_score=0\n",
    "            max_id=-1\n",
    "            for q in range(i,j):  \n",
    "                score = weight_matrix[i,q,1,1]+weight_matrix[q+1,j,0,1]+G[j, i]\n",
    "                if score>max_score:\n",
    "                    max_score=score\n",
    "                    max_id=q\n",
    "            weight_matrix[i,j,0,0]=max_score\n",
    "            selection_matrix[i,j,0,0] = max_id\n",
    "\n",
    "            ##d=1, c=0\n",
    "            max_score=0\n",
    "            max_id=-1\n",
    "            for q in range(i,j):  \n",
    "                score = weight_matrix[i,q,1,1]+weight_matrix[q+1,j,0,1]+G[i, j]\n",
    "                if score>max_score:\n",
    "                    max_score=score\n",
    "                    max_id=q\n",
    "            weight_matrix[i,j,1,0]=max_score\n",
    "            selection_matrix[i,j,1,0] = max_id\n",
    "\n",
    "            ##d=0, c=1\n",
    "            max_score=0\n",
    "            max_id=-1\n",
    "            for q in range(i,j+1):  \n",
    "                score = weight_matrix[i,q,0,1]+weight_matrix[q,j,0,0]\n",
    "                if score>max_score:\n",
    "                    max_score=score\n",
    "                    max_id=q\n",
    "            weight_matrix[i,j,0,1]=max_score\n",
    "            selection_matrix[i,j,0,1] = max_id\n",
    "\n",
    "            ##d=1, c=1\n",
    "            max_score=0\n",
    "            max_id=-1\n",
    "            for q in range(i,j+1):  \n",
    "                score = weight_matrix[i,q,1,0]+weight_matrix[q,j,1,1]\n",
    "                if score>max_score:\n",
    "                    max_score=score\n",
    "                    max_id=q\n",
    "            weight_matrix[i,j,1,1]=max_score\n",
    "            selection_matrix[i,j,1,1] = max_id\n",
    "    dep_tree=Traceback(selection_matrix,0,num_v-1,1,1)\n",
    "    return dep_tree\n",
    "\n",
    "    \n",
    "def stringify(dictionary, offset):\n",
    "    string_list = []\n",
    "    for key in dictionary.keys():\n",
    "        for element in dictionary[key]:\n",
    "            string_list.append(f\"{key+offset}-{element+offset}\")\n",
    "    return string_list\n",
    "\n",
    "def Traceback(selection_matrix,i,j,d,c):\n",
    "    if i==j:\n",
    "        return {}\n",
    "    q=int(selection_matrix[i,j,d,c])\n",
    "    if d==1 and c==1:\n",
    "        left_result = Traceback(selection_matrix,i,q,1,0)\n",
    "        right_result= Traceback(selection_matrix,q,j,1,1)\n",
    "        current_dep=merge_dict(left_result,right_result)\n",
    "    elif d==0 and c==1:\n",
    "        left_result = Traceback(selection_matrix,i,q,0,1)\n",
    "        right_result= Traceback(selection_matrix,q,j,0,0)\n",
    "        current_dep=merge_dict(left_result,right_result)\n",
    "\n",
    "    elif d==1 and c==0:\n",
    "        left_result = Traceback(selection_matrix,i,q,1,1)\n",
    "        right_result= Traceback(selection_matrix,q+1,j,0,1)\n",
    "        current_dep=merge_dict(left_result,right_result)\n",
    "        current_dep=merge_dict(current_dep,{i:[j]})\n",
    "    elif d==0 and c==0:\n",
    "        left_result = Traceback(selection_matrix,i,q,1,1)\n",
    "        right_result= Traceback(selection_matrix,q+1,j,0,1)\n",
    "        current_dep=merge_dict(left_result,right_result)\n",
    "        current_dep=merge_dict(current_dep,{j:[i]})\n",
    "\n",
    "    return current_dep\n",
    "\n",
    "def merge_dict(dict1,dict2):\n",
    "    for k in dict2.keys():\n",
    "        if k in dict1.keys():\n",
    "            dict1[k].extend(dict2[k])\n",
    "        else:\n",
    "            dict1[k] = dict2[k]\n",
    "    return dict1\n",
    "\n",
    "# Doubly linked binary consituency Tree definition        \n",
    "class Node(object):\n",
    "    def __init__(self, idx, nuclearity):\n",
    "        self.idx = idx\n",
    "        self.nuclearity = nuclearity\n",
    "        self.parent = None\n",
    "        self.children = [None, None]\n",
    "\n",
    "    def add_child(self,child, branch):\n",
    "        child.parent = self\n",
    "        self.children[branch] = child\n",
    "\n",
    "# Doubly linked dependency Tree definition\n",
    "class Dep_Node(object):\n",
    "    def __init__(self, idx):\n",
    "        self.idx = idx\n",
    "        self.parent = None\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, child):\n",
    "        child.parent = self\n",
    "        self.children.append(child)\n",
    "        \n",
    "# Transform brackets to in-memory tree \n",
    "def generate_const_tree(bracket_file):\n",
    "    nodes_stack = []\n",
    "    for node in bracket_file:\n",
    "        node_index_tupel = node[0]\n",
    "        node_nuclearity = node[1]\n",
    "        # Check if node is a leaf\n",
    "        if node_index_tupel[0] == node_index_tupel[1]:\n",
    "            nodes_stack.append(Node(idx = node_index_tupel, \n",
    "                                    nuclearity = node_nuclearity))\n",
    "        # Add children for internal nodes\n",
    "        else:\n",
    "            tmp_node = Node(idx = node_index_tupel, nuclearity = node_nuclearity)\n",
    "            tmp_node.add_child(nodes_stack.pop(), branch = 1)\n",
    "            tmp_node.add_child(nodes_stack.pop(), branch = 0)\n",
    "            nodes_stack.append(tmp_node)\n",
    "    # Join last two nodes in root    \n",
    "    root_node = Node(idx = 'Root', nuclearity = 'Root')\n",
    "    root_node.branch = 0\n",
    "    root_node.add_child(nodes_stack.pop(), branch = 1)\n",
    "    root_node.add_child(nodes_stack.pop(), branch = 0)\n",
    "#     print(\"POT\")\n",
    "#     post_order_traversal(root_node)\n",
    "    return root_node\n",
    "       \n",
    "def post_order_traversal(node):\n",
    "    if node == None:\n",
    "        return\n",
    "    else:\n",
    "        post_order_traversal(node.children[0])\n",
    "        post_order_traversal(node.children[1])\n",
    "    print(node.idx)\n",
    "    \n",
    "def const_to_dep_tree_li14(node):\n",
    "    const_leaves = get_leaf_nodes(node)\n",
    "    dep_tree={}\n",
    "    for leaf in const_leaves:\n",
    "        p = find_top_node(leaf)\n",
    "        if p.nuclearity=='Root':\n",
    "            root = leaf.idx[0]\n",
    "        else:\n",
    "            head = find_head_edu(p.parent)\n",
    "            if head not in dep_tree:\n",
    "                dep_tree[head]=[]\n",
    "            dep_tree[head].append(leaf.idx[0])\n",
    "    return dep_tree\n",
    "\n",
    "def find_top_node(e):\n",
    "    C=e\n",
    "    p = C.parent\n",
    "    nucleus_child = [p.children[i] for i in range(len(p.children)) if p.children[i].nuclearity=='Nucleus']\n",
    "    while nucleus_child[0]==C and not p.nuclearity=='Root':\n",
    "        C = p\n",
    "        p = C.parent\n",
    "        nucleus_child = [p.children[i] for i in range(len(p.children)) if p.children[i].nuclearity=='Nucleus']\n",
    "    if p.nuclearity=='Root'and nucleus_child[0]==C:\n",
    "        C=p\n",
    "    return C\n",
    "\n",
    "def find_head_edu(p):\n",
    "    while p.children!=[None, None]:\n",
    "        nucleus_child = [p.children[i] for i in range(len(p.children)) if p.children[i].nuclearity=='Nucleus']\n",
    "        p = nucleus_child[0]\n",
    "    return p.idx[0]\n",
    "\n",
    "def get_leaf_nodes(node):\n",
    "    if node.children == [None, None]:\n",
    "        return [node]\n",
    "    else:\n",
    "        leaves = []\n",
    "        for child_node in node.children:\n",
    "            leaves.extend(get_leaf_nodes(child_node))\n",
    "        return leaves\n",
    "\n",
    "def calculate_UAS(gold_tree, pred_tree):\n",
    "    overlap = len(list(set(gold_tree) & set(pred_tree)))\n",
    "    samples = len(gold_tree)\n",
    "    return {\"eisner_matches\": overlap, \"eisner_samples\": samples}\n",
    "      \n",
    "def calculate_confident_score(pred_tree, edu_mat, dataname):\n",
    "    # confidence score calculation C=averaged attn score of predicted decisions d\n",
    "    decisions = [d.split('-') for d in pred_tree]\n",
    "    attn_scores = 0.0\n",
    "    for d in decisions:\n",
    "        if dataname in ['gumconv', 'rst']:#offset for gum\n",
    "            d[0] = int(d[0]) - 1\n",
    "            d[1] = int(d[1]) - 1 \n",
    "        attn_scores += edu_mat[int(d[0]), int(d[1])]\n",
    "    confi_score = round((attn_scores / len(decisions)).item(), 4)\n",
    "    return {\"confident_score\": confi_score}\n",
    "\n",
    "def exec_eisner(curr_file, matrix, base_save_path, model_extension, layer, head, dataname, add_confi):\n",
    "    filename = os.path.basename(curr_file).split(\".\")[0]\n",
    "    # Skip if exists\n",
    "    if os.path.exists(os.path.join(base_save_path, model_extension, \"eisner_unlabelled_attachments\", f\"{layer}_{head}_{filename}.brackets\")):\n",
    "        with open(os.path.join(base_save_path, model_extension, \"eisner_unlabelled_attachments\", f\"{layer}_{head}_{filename}.brackets\"), \"r\") as f:\n",
    "            output = f.read().split(\"\\n\")\n",
    "    else:\n",
    "        output = stringify(eisner(matrix), 0)\n",
    "        with open(os.path.join(base_save_path, model_extension, \"eisner_unlabelled_attachments\", f\"{layer}_{head}_{filename}.brackets\"), \"w\") as f:\n",
    "            f.write(\"\\n\".join(output))\n",
    "    \n",
    "    # Comparison\n",
    "    # TODO: gold_tree is a dictionary with key = file_id, value = {1: [2,3,6], 2: [3], ...} where keys are head index, elements in list are dependents index\n",
    "    gold_tree = GOLD_TREE[curr_file]\n",
    "    gold_tree = stringify(gold_tree, 0) \n",
    "    ret = calculate_UAS(gold_tree, output)\n",
    "    \n",
    "    if add_confi:# li: add avg attn scores as confi score\n",
    "        confi_score = calculate_confident_score(output, matrix, dataname)\n",
    "        ret.update(confi_score)\n",
    "\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models can be found at Huggingface\n",
    "BART_large = {\"d_name\": \"BART\", \"name\":\"facebook/bart-large\", \"prefix_len\":1, \"postfix_len\":1, \"max_input_size\":1024, \"batch_size\": 1,\n",
    "              \"start_token\":0, \"end_token\":2, \"has_token_type_ids\": False, \"attn_keyword\": \"encoder_attentions\", \"model\": BartModel}\n",
    "BART_large_cnn = {\"d_name\": \" + CNN\", \"name\":\"facebook/bart-large-cnn\", \"prefix_len\":1, \"postfix_len\":1, \"max_input_size\":1024, \"batch_size\": 1,\n",
    "            \"start_token\":0, \"end_token\":2, \"has_token_type_ids\": False, \"attn_keyword\": \"encoder_attentions\", \"model\": BartModel}\n",
    "BART_large_samsum = {\"d_name\": \" + SAMSUM\", \"name\":\"linydub/bart-large-samsum\", \"prefix_len\":1, \"postfix_len\":1, \"max_input_size\":1024, \"batch_size\": 1,\n",
    "            \"start_token\":0, \"end_token\":2, \"has_token_type_ids\": False, \"attn_keyword\": \"encoder_attentions\", \"model\": BartModel}\n",
    "BART_large_squad2 = {\"d_name\": \" + SQuAD2\", \"name\":\"phiyodr/bart-large-finetuned-squad2\", \"prefix_len\":1, \"postfix_len\":1, \"max_input_size\":1024, \"batch_size\": 1,\n",
    "            \"start_token\":0, \"end_token\":2, \"has_token_type_ids\": False, \"attn_keyword\": \"encoder_attentions\", \"model\": BartModel}\n",
    "RoBERTa_large = {\"d_name\": \"ROBERTA-large\", \"name\":\"roberta-large\", \"prefix_len\":1, \"postfix_len\":1, \"max_input_size\":512, \"batch_size\": 1,\n",
    "            \"start_token\":0, \"end_token\":2, \"has_token_type_ids\": True, \"attn_keyword\": \"attentions\", \"model\": RobertaModel} #large: 24 x 16, https://huggingface.co/roberta-large/blob/main/config.json        \n",
    "DialoGPT = {\"d_name\": \"DIALOGPT\", \"name\":\"microsoft/DialoGPT-small\", \"prefix_len\":1, \"postfix_len\":1, \"max_input_size\":1024, \"batch_size\": 1,\n",
    "            \"start_token\":50256, \"end_token\":50256, \"has_token_type_ids\": True, \"attn_keyword\": \"attentions\", \"model\": GPT2LMHeadModel} #12 x 12, https://huggingface.co/microsoft/DialoGPT-small/blob/main/config.json\n",
    "DialogLED_large = {\"d_name\": \"DialogLED\", \"name\":\"MingZhong/DialogLED-large-5120\", \"prefix_len\":1, \"postfix_len\":1, \"max_input_size\":1024, \"batch_size\": 1,\n",
    "            \"start_token\":0, \"end_token\":2, \"has_token_type_ids\": False, \"attn_keyword\": \"encoder_attentions\", \"model\": LEDModel} # https://huggingface.co/MingZhong/DialogLED-large-5120/blob/main/config.json\n",
    "# Models that i fine-tuned\n",
    "BART_large_dd_t2m_mix_shuf_all = {\"d_name\": \" + dd_t2m_mix-shuf-all\", \"name\":\"cli/bart-large_dd_t2m_mix-shuf-all/checkpoint-26595\", \"prefix_len\":1, \"postfix_len\":1, \"max_input_size\":1024, \"batch_size\": 1,\n",
    "            \"start_token\":0, \"end_token\":2, \"has_token_type_ids\": False, \"attn_keyword\": \"encoder_attentions\", \"model\": BartModel}\n",
    "BART_large_stac_t2m_mix_shuf_all = {\"d_name\": \" + stac_t2m_mix-shuf-all\", \"name\":\"cli/bart-large_stac_t2m_mix-shuf-all/checkpoint-13258\", \"prefix_len\":1, \"postfix_len\":1, \"max_input_size\":1024, \"batch_size\": 1,\n",
    "            \"start_token\":0, \"end_token\":2, \"has_token_type_ids\": False, \"attn_keyword\": \"encoder_attentions\", \"model\": BartModel}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXECUTE = False\n",
    "datasets = [\"stac\", \"rst\", \"gum\"]\n",
    "model_params_all = [BART_large]\n",
    "\n",
    "curr_core = 0\n",
    "random_init = True\n",
    "orig_parseval = True\n",
    "\n",
    "if EXECUTE:\n",
    "    for model_params in model_params_all:\n",
    "        for dataset in datasets:\n",
    "            if \"bart\" in model_params[\"name\"]:\n",
    "                layers = (0, 12)\n",
    "                heads = (0, 16)\n",
    "            elif \"bert\" in model_params[\"name\"]:\n",
    "                layers = (0, 12)\n",
    "                heads = (0, 12) \n",
    "            # TODO: input data: change to your dataset repo which contains all the files to parse. Each file contains one utterance/line\n",
    "            if dataset == \"rst\":\n",
    "                file_path = \"path2rst_folder/\" \n",
    "            elif dataset == \"gum\":\n",
    "                file_path = \"path2gum_folder/\"\n",
    "            elif dataset == \"stac\":\n",
    "                file_path = \"path2stac_folder/\"\n",
    "            base_save_path = \"path2model_results/\"\n",
    "\n",
    "            #Initialize files and folders if they don't exist already\n",
    "            if random_init:\n",
    "                base_save_path = os.path.join(base_save_path, \"rand_init\")\n",
    "            if not os.path.exists(os.path.join(base_save_path, model_params[\"name\"])):\n",
    "                os.makedirs(os.path.join(base_save_path, model_params[\"name\"], \"attention_matrices\"))\n",
    "                os.makedirs(os.path.join(base_save_path, model_params[\"name\"], \"eisner_unlabelled_attachments\"))\n",
    "                open(os.path.join(base_save_path, model_params[\"name\"], \"results.jsonl\"), 'a').close()\n",
    "\n",
    "            # Execution\n",
    "            for layer in range(0, layers[1]):\n",
    "                for head in range(0, heads[1]): \n",
    "                    oneres = execution(curr_core, model_params, file_path, base_save_path, layer, head, random_init, orig_parseval, dataset)\n",
    "                    if orig_parseval:\n",
    "                        orig = \"_orig_parseval\"\n",
    "                    else:\n",
    "                        orig = \"\"\n",
    "                    with open(os.path.join(base_save_path, model_params[\"name\"], f\"results{orig}.jsonl\"), 'a') as f:\n",
    "                        for datapoint in oneres:\n",
    "                            datapoint = json.dumps(datapoint)\n",
    "                            f.write(datapoint+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize(title, heatmap, layers, heads, model_name):\n",
    "    fig = plt.figure()\n",
    "    fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "    ax.set_xticks(np.arange(start=-1, stop=heads, step=1))\n",
    "    ax.set_yticks(np.arange(start=layers, stop=-1, step=-1))\n",
    "    ax.set_xticklabels(np.arange(start=-1, stop=heads, step=1))\n",
    "    ax.set_yticklabels(np.arange(start=layers, stop=-1, step=-1))\n",
    "    heatplot = ax.imshow(heatmap, cmap='BuPu', interpolation='nearest', vmin=heatmap.min(), vmax=heatmap.max())\n",
    "    tick_spacing = 1\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "    for i in range(len(heatmap)):\n",
    "        for j in range(len(heatmap[0])):\n",
    "            text = ax.text(j, i, heatmap[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'./figures/{model_name}_{title}.png')\n",
    "\n",
    "def visualize_pictogram(title, heatmap, layers, heads, starts):\n",
    "    fig = plt.figure()\n",
    "    fig, ax = plt.subplots(1,1, figsize=(2.5,2.5))\n",
    "    heatplot = ax.imshow(heatmap, cmap='BuPu', interpolation='nearest', vmin=heatmap.min(), vmax=heatmap.max())\n",
    "    ax.axes.xaxis.set_ticks([])\n",
    "    ax.axes.yaxis.set_ticks([])\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'./pictogram/{starts}_{title}.png', format='png', bbox_inches='tight')\n",
    "    \n",
    "def get_model_statistics_pretty(heatmap_span, heatmap_nuc, heatmap_eisner, name):\n",
    "    heatmap_span = [item for sublist in heatmap_span for item in sublist]\n",
    "    heatmap_nuc = [item for sublist in heatmap_nuc for item in sublist]\n",
    "    heatmap_eisner = [item for sublist in heatmap_eisner for item in sublist]\n",
    "    heatmap_span.sort()\n",
    "    heatmap_nuc.sort()\n",
    "    heatmap_eisner.sort()\n",
    "    print(f\"Model {name}\")\n",
    "    print(f\"Span (avg, med, min, max):\\t {round(sum(heatmap_span)/len(heatmap_span), 2)}\\t{heatmap_span[int(len(heatmap_span)/2)]}\\t{heatmap_span[0]}\\t{heatmap_span[-1]}\")\n",
    "    print(f\"Nuc (avg, med, min, max):\\t {round(sum(heatmap_nuc)/len(heatmap_nuc), 2)}\\t{heatmap_nuc[int(len(heatmap_nuc)/2)]}\\t{heatmap_nuc[0]}\\t{heatmap_nuc[-1]}\")\n",
    "    print(f\"Eis (avg, med, min, max):\\t {round(sum(heatmap_eisner)/len(heatmap_eisner), 2)}\\t{heatmap_eisner[int(len(heatmap_eisner)/2)]}\\t{heatmap_eisner[0]}\\t{heatmap_eisner[-1]}\")\n",
    "    \n",
    "def get_model_statistics(heatmap_span, heatmap_nuc, heatmap_eisner, name):\n",
    "    heatmap_span = [item for sublist in heatmap_span for item in sublist]\n",
    "    heatmap_nuc = [item for sublist in heatmap_nuc for item in sublist]\n",
    "    heatmap_eisner = [item for sublist in heatmap_eisner for item in sublist]\n",
    "    heatmap_span.sort()\n",
    "    heatmap_nuc.sort()\n",
    "    heatmap_eisner.sort()\n",
    "    print(f\"{name} & {round(heatmap_span[0], 1)} & {round(heatmap_span[int(len(heatmap_span)/2)], 1)} & {round(sum(heatmap_span)/len(heatmap_span), 1)} & {round(heatmap_span[-1], 1)} & & {round(heatmap_eisner[0], 1)} & {round(heatmap_eisner[int(len(heatmap_eisner)/2)], 1)} & {round(sum(heatmap_eisner)/len(heatmap_eisner), 1)} & {round(heatmap_eisner[-1], 1)}\")\n",
    "    \n",
    "model_params_all = [BART_large]\n",
    "random_init = False \n",
    "BOTTOM_N_LAYERS = 12\n",
    "starts_with = \"GUM\"\n",
    "orig_parseval = True\n",
    "\n",
    "if orig_parseval:\n",
    "    orig = \"_orig_parseval\"\n",
    "else:\n",
    "    orig = \"\"\n",
    "\n",
    "for model_params in model_params_all:\n",
    "    if \"bart\" in model_params[\"name\"]:\n",
    "        HEADS = 16\n",
    "    elif \"bert\" in model_params[\"name\"]:\n",
    "        HEADS = 12\n",
    "\n",
    "    if random_init:\n",
    "        rand_txt = \"_rand_init\"\n",
    "        base_save_path = \"path2model_results/rand_init\"\n",
    "    else:\n",
    "        rand_txt = \"\"\n",
    "        base_save_path = \"path2model_results\"\n",
    "\n",
    "    save_file_path = os.path.join(base_save_path, model_params[\"name\"], f\"results{orig}.jsonl\")\n",
    "\n",
    "    with open(save_file_path, \"r\") as f:\n",
    "        cell_performance = {}\n",
    "        for line in f:\n",
    "            line = json.loads(line)\n",
    "            layer = line['layer']\n",
    "            head = line['head']\n",
    "            file = line['file_name']\n",
    "            if not file.startswith(starts_with): continue\n",
    "            eisner_matches = line['eisner_matches']\n",
    "            eisner_samples = line['eisner_samples']\n",
    "            if f\"{layer}_{head}\" not in cell_performance:\n",
    "                cell_performance[f\"{layer}_{head}\"] = {\"eisner_matches\":0, \n",
    "                                                       \"eisner_samples\":0, \"layer\":-1, \"head\":-1}\n",
    "            cell_performance[f\"{layer}_{head}\"][\"eisner_matches\"] += eisner_matches\n",
    "            cell_performance[f\"{layer}_{head}\"][\"eisner_samples\"] += eisner_samples\n",
    "            cell_performance[f\"{layer}_{head}\"][\"layer\"] = layer\n",
    "            cell_performance[f\"{layer}_{head}\"][\"head\"] = head\n",
    "\n",
    "    heatmap_span = np.zeros((BOTTOM_N_LAYERS, HEADS))\n",
    "    heatmap_nuc = np.zeros((BOTTOM_N_LAYERS, HEADS))\n",
    "    heatmap_eisner = np.zeros((BOTTOM_N_LAYERS, HEADS))\n",
    "    for key in cell_performance.keys():\n",
    "        eisner_matches = cell_performance[key][\"eisner_matches\"]\n",
    "        eisner_samples = cell_performance[key][\"eisner_samples\"]\n",
    "        layer = cell_performance[key][\"layer\"]\n",
    "        head = cell_performance[key][\"head\"]\n",
    "        heatmap_eisner[(BOTTOM_N_LAYERS-1)-layer, head] = round((eisner_matches/eisner_samples)*100, 2)\n",
    "\n",
    "    print(f\"Samples starting with {starts_with} only (selector for GUM/RST-DT/both)\")\n",
    "    get_model_statistics(heatmap_span, heatmap_nuc, heatmap_eisner, model_params['d_name'].split('/')[-1])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input file format\n",
    "\n",
    "For instance, in a \"pilot02_1.out.edus\" file, you will have:\n",
    "\n",
    "```\n",
    "Cat: so I'd like to build a road... \n",
    "Cat: where do i press? \n",
    "william: to build? \n",
    "william: at the bottom next to \"road\" etc. \n",
    "Thomas: if you have 1 wood, 1 clay,  \n",
    "Thomas: it is at the bottom of the screen \n",
    "Cat: okay, I see... \n",
    "Cat: anyone wants an ore? \n",
    "Thomas: nope \n",
    "william: no \n",
    "Cat: for a wood \n",
    "william: you need 10 victory points to win.  \n",
    "william: a settlement gives you 1 VP,  \n",
    "william: a city gives you 2 VP \n",
    "Cat: okay, thanks \n",
    "```\n",
    "\n",
    "## Gold file format\n",
    "\n",
    "In variable ``GOLD_TREE``, gold_tree is an object of type dictionary with key = file_id, value = {1: [2,3,6], 2: [3], ...} where keys are head index and elements in list are dependents index. \n",
    "\n",
    "\n",
    "## Output file format\n",
    "\n",
    "For instance, in a \"0_0_pilot02_1.brackets\" file, you will have:\n",
    "```\n",
    "0-1\n",
    "1-2\n",
    "2-3\n",
    "3-4\n",
    "4-5\n",
    "...\n",
    "```\n",
    "These are pairs of EDU indices that are predicted to be linked.\n",
    "\n",
    "## Fine-tuned BART model on sentence ordering, ref to github ReBART:\n",
    "https://github.com/fabrahman/ReBART\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc1594c03ca7590647b39727ff00c5d11b8b0972478e419c43d960d3b0ae632f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
